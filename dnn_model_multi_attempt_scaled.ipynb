{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1668377780958,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"MwAnnaX7eVvz","outputId":"0c8eb95f-74f4-4be9-ef5c-22d896cd88cb"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1668377780959,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"vKVmF0APRk7-"},"outputs":[],"source":["import sys\n","path_workspace = '/content/r2_learning_calibration'\n","path_source = '/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs'\n","sys.path.append(path_source)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":29607,"status":"ok","timestamp":1668377810557,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"e5BWVDa9Fk5z"},"outputs":[],"source":["!mkdir '/content/r2_learning_calibration'\n","!mkdir '/content/r2_learning_calibration/data'\n","\n","# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/test_6hour_unloaded.zip\" -d '/content/r2_learning_calibration/data'\n","\n","# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/test_6hour_500g_loaded.zip\" -d '/content/r2_learning_calibration/data'\n","\n","!unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/test_no_homing_in_train.zip\" -d '/content/r2_learning_calibration/data'\n","\n","# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/test_with_homing_in_train.zip\" -d '/content/r2_learning_calibration/data'\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2653,"status":"ok","timestamp":1668377813200,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"WDDRDTWPRvSJ"},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits import mplot3d\n","\n","import heapq\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras import regularizers\n","\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1668384676757,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"bRpd_InUIkzA"},"outputs":[],"source":["# parameter list\n","# file_train_list_ = ['data_record_yz_05_1.csv', 'data_record_yz_03_1.csv', 'data_record_yz_025_1.csv', 'data_record_yz_02_1.csv', 'data_record_yz_016_1.csv'] # for 0g\n","file_train_list_ = ['data_record_yz_05.csv', 'data_record_yz_03.csv', 'data_record_yz_025.csv', 'data_record_yz_02.csv', 'data_record_yz_016.csv'] # for 500g and homing decay\n","# file_train_list_ = ['data_record_yz_05_hm0.csv', 'data_record_yz_03_hm1.csv', 'data_record_yz_025_hm1.csv', 'data_record_yz_02_hm1.csv', 'data_record_yz_016_hm0.csv'] # for homing in train\n","\n","\n","\n","# file_test_list_ = ['data_record_rand1200_h0.csv', 'data_record_rand1200_h1.csv', 'data_record_rand1200_h2.csv', 'data_record_rand1200_h3.csv', 'data_record_rand1200_h4.csv', 'data_record_rand1200_h5.csv']\n","file_test_list_ = ['data_record_rand1200_hm0.csv', 'data_record_rand1200_hm1.csv', 'data_record_rand1200_hm2.csv', 'data_record_rand1200_hm3.csv', 'data_record_rand1200_hm4.csv', 'data_record_rand1200_hm5.csv']\n","\n","\n","#hidden_layers_ = [[1000], [500], [200]]\n","hidden_layers_ = [[600], [500], [400]]\n","learning_rate = 0.0005\n","batch_size = 1024\n","epochs = 600\n","\n","kernel_regu_l1 = 1e-5  #defult 1e-5, cur best 5e-5\n","kernel_regu_l2 = 1e-4  #defult 1e-4, cur best 5e-4\n","bias_regu = 1e-4       #defult 1e-4, cur best 5e-4\n","activity_regu = 1e-5   #defult 1e-5, cur best 5e-5\n","\n","# feature_list = [np.arange(12,82),np.arange(114,130),np.arange(194,212)]\n","# feature_list = [np.arange(12,82),np.arange(98,130), np.arange(146,212), np.arange(228,252)] # this feature set excluded encoder values, motor pose, encoder offsets\n","# feature_list = [np.arange(12,82),np.arange(98,133), np.arange(146,212), np.arange(228,252)] # included mpos for joint 123, works\n","# feature_list = [np.arange(12,82),np.arange(98,133), np.arange(134,212), np.arange(228,252)] ##### only excluded 'joint 4' the void joint, works\n","# feature_list = [np.arange(29,82),np.arange(98,130), np.arange(146,178), np.arange(210,212), np.arange(228,252)] # No jpos, jpos_d, mpo, mpos_d\n","# feature_list = [np.arange(12,13), # time stamp\n","#                 np.arange(13,29), # jpos\n","#                 np.arange(29,30), # run level\n","#                 np.arange(30,31), # sub level\n","#                 np.arange(31,32), # last_seq\n","#                 np.arange(32,34), # type\n","#                 np.arange(34,40), # pos\n","#                 np.arange(40,58), # ori\n","#                 np.arange(58,76), # ord_d\n","#                 np.arange(76,82), # pos_d\n","#                 #np.arange(82,98), # enc_val\n","#                 np.arange(98,114), # dac_val\n","#                 np.arange(114,130), # tau\n","#                 np.arange(130,133),np.arange(133,146), # mpos\n","#                 np.arange(146,162), #  mvel\n","#                 np.arange(162,178), # jvel\n","#                 np.arange(178,194), # mpos_d\n","#                 np.arange(194,210), # jpos_d\n","#                 np.arange(210,212), # grasp_d\n","#                 #np.arange(212,228), # enc_offset\n","#                 np.arange(228,240), # jac_vel\n","#                 np.arange(240,252)] # jac_f\n","\n","\n","feature_list = [np.arange(12,13), # time stamp\n","                np.arange(13,29), # jpos\n","                np.arange(29,30), # run level\n","                np.arange(30,31), # sub level\n","                np.arange(31,32), # last_seq\n","                np.arange(32,34), # type\n","                np.arange(34,40), # pos\n","                np.arange(40,58), # ori\n","                np.arange(58,76), # ord_d\n","                np.arange(76,82), # pos_d\n","                #np.arange(82,98), # enc_val\n","                np.arange(98,114), # dac_val\n","                np.arange(114,130), # tau\n","                np.arange(130,133),np.arange(133,146), # mpos\n","                np.arange(146,162), #  mvel\n","                np.arange(162,178), # jvel\n","                np.arange(178,194), # mpos_d\n","                np.arange(194,210), # jpos_d\n","                np.arange(210,212), # grasp_d\n","                #np.arange(212,228), # enc_offset\n","                np.arange(228,240), # jac_vel\n","                np.arange(240,252)] # jac_f\n","\n","\n","noise_type = 'temp'\n","noise_list = [np.arange(98,114) # dac_val\n","              ] \n","\n","feature_select = np.array([])\n","for feature in feature_list:\n","  feature_select = np.hstack([feature_select, feature])\n","feature_select = feature_select.astype(int)\n","feature_dim = feature_select.size\n","\n","noise_select = np.array([])\n","for noise in noise_list:\n","  noise_select = np.hstack([noise_select, noise])\n","noise_select = noise_select.astype(int)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":101302,"status":"ok","timestamp":1668384779289,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"ZhfK8eW9_pPn","outputId":"70e3c77d-d613-4628-9608-a55c775f1d09"},"outputs":[],"source":["# import data\n","path_data = '/content/r2_learning_calibration/data/'\n","\n","\n","# Load training data\n","first_call = True\n","for file_training in file_train_list_:\n","  data = np.loadtxt(path_data + file_training, delimiter = ',')\n","  # jpos = data[:, 13:29]\n","  # jpos_d = data[:, 194:210]\n","  # data[:, 13:29] = jpos_d\n","  # data[:, 194:210] = jpos\n","  # data[:, 194:210] = data[:, 13:29] # [temp] for tesing jpos and jpos_d\n","  if first_call:\n","    data_train = data\n","    first_call = False\n","  else:\n","    data_train = np.vstack((data_train, data))\n","\n","# Load testing data\n","data_test_list = []\n","for file_test in file_test_list_:\n","  data = np.loadtxt(path_data + file_test, delimiter = ',')\n","  # jpos = data[:, 13:29]\n","  # jpos_d = data[:, 194:210]\n","  # data[:, 13:29] = jpos_d\n","  # data[:, 194:210] = jpos\n","  # data[:, 194:210] = data[:, 13:29] # [temp] for tesing jpos and jpos_d\n","  data_test_list.append(data)\n","\n","# load less validation data for faster training\n","data_vali = np.loadtxt(path_data + file_test_list_[0], delimiter = ',')\n","data_vali = data_vali[:10000,:]\n","\n","print('Training data loaded, shape:')\n","print(data_train.shape)\n","print('Test data loaded, shape:')\n","for data_test in data_test_list:\n","  print(data_test.shape)\n","\n","rad2deg = 180.0/np.pi\n","deg2rad = np.pi/180.0\n","\n","# load the data for visualization only, joint 1 and 2 will be in deg, joint 3 in m\n","time_line = data_train[:,0]\n","ext_jpos_1 = data_train[:,1] *rad2deg   # joint pose from external joint encoders\n","ext_jpos_2 = data_train[:,2] *rad2deg\n","ext_jpos_3 = data_train[:,3]\n","rs_jpos_1 = data_train[:,13]            # joint pose from ravenstate\n","rs_jpos_2 = data_train[:,14]\n","rs_jpos_3 = data_train[:,15] *deg2rad\n","\n","rs_jpos_1_lc = rs_jpos_1 + (np.mean(ext_jpos_1)-np.mean(rs_jpos_1))\n","rs_jpos_2_lc = rs_jpos_2 + (np.mean(ext_jpos_2)-np.mean(rs_jpos_2))\n","rs_jpos_3_lc = rs_jpos_3 + (np.mean(ext_jpos_3)-np.mean(rs_jpos_3))\n","\n","bias_jpos_1 = np.mean(ext_jpos_1)-np.mean(rs_jpos_1)\n","bias_jpos_2 = np.mean(ext_jpos_2)-np.mean(rs_jpos_2)\n","bias_jpos_3 = np.mean(ext_jpos_3)-np.mean(rs_jpos_3)\n","\n","\n","fig = plt.figure()\n","ax = plt.axes(projection='3d')\n","ax.plot3D(ext_jpos_1, ext_jpos_2, ext_jpos_3)\n","ax.plot3D(rs_jpos_1, rs_jpos_2, rs_jpos_3)\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Traning 3D traj')\n","\n","fig = plt.figure()\n","plt.plot(time_line, ext_jpos_1)\n","plt.plot(time_line, rs_jpos_1, 'r--')\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Joint 1 raw')\n","\n","fig = plt.figure()\n","plt.plot(time_line, ext_jpos_2)\n","plt.plot(time_line, rs_jpos_2, 'r--')\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Joint 2 raw')\n","\n","fig = plt.figure()\n","plt.plot(time_line, ext_jpos_3)\n","plt.plot(time_line, rs_jpos_3, 'r--')\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Joint 3 raw')\n","\n","fig = plt.figure()\n","plt.plot(time_line, ext_jpos_1)\n","plt.plot(time_line, rs_jpos_1_lc, 'r--')\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Joint 1 linear correct')\n","\n","fig = plt.figure()\n","plt.plot(time_line, ext_jpos_2)\n","plt.plot(time_line, rs_jpos_2_lc, 'r--')\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Joint 2 linear correct')\n","\n","fig = plt.figure()\n","plt.plot(time_line, ext_jpos_3)\n","plt.plot(time_line, rs_jpos_3_lc, 'r--')\n","plt.legend(('ext_enc','jpos_raven_state'))\n","plt.title('Joint 3 linear correct')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gknVOgFAJKNlO0qwCVDgSoSGxj43jDON"},"executionInfo":{"elapsed":2070087,"status":"ok","timestamp":1668386849374,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"K_0TuFQA4E67","outputId":"340b5fb2-b5fe-4b25-be8e-333ee1c3ca16"},"outputs":[],"source":["repeat_times = 5\n","\n","\n","# Training\n","x_train = data_train[:, feature_select]\n","x_train[:,0] = data_train[:, 0] # change the time stamp to relative time (start with 0)\n","y_train = data_train[:, 1:4]\n","\n","ext_jpos_1 = data_train[:,1] *rad2deg   # joint pose from external joint encoders\n","ext_jpos_2 = data_train[:,2] *rad2deg\n","ext_jpos_3 = data_train[:,3]\n","rs_jpos_1 = data_train[:,13]            # joint pose from ravenstate\n","rs_jpos_2 = data_train[:,14]\n","rs_jpos_3 = data_train[:,15] *deg2rad\n","\n","rs_jpos_1_lc = rs_jpos_1 + (np.mean(ext_jpos_1)-np.mean(rs_jpos_1))\n","rs_jpos_2_lc = rs_jpos_2 + (np.mean(ext_jpos_2)-np.mean(rs_jpos_2))\n","rs_jpos_3_lc = rs_jpos_3 + (np.mean(ext_jpos_3)-np.mean(rs_jpos_3))\n","\n","bias_jpos_1 = np.mean(ext_jpos_1)-np.mean(rs_jpos_1)\n","bias_jpos_2 = np.mean(ext_jpos_2)-np.mean(rs_jpos_2)\n","bias_jpos_3 = np.mean(ext_jpos_3)-np.mean(rs_jpos_3)\n","\n","y_train[:,0] = ext_jpos_1 - rs_jpos_1\n","y_train[:,1] = ext_jpos_2 - rs_jpos_2\n","y_train[:,2] = ext_jpos_3 - rs_jpos_3\n","\n","# scale the training set\n","scaler_x_train = StandardScaler()\n","scaler_y_train = StandardScaler()\n","x_train_scaled = scaler_x_train.fit_transform(x_train)\n","y_train_scaled = scaler_y_train.fit_transform(y_train)\n","\n","# Validation\n","x_vali = data_vali[:, feature_select]\n","x_vali[:,0] = data_vali[:, 0] # change the time stamp to relative time (start with 0)\n","y_vali = data_vali[:, 1:4]\n","y_vali[:,0] = data_vali[:,1] *rad2deg - data_vali[:,13]\n","y_vali[:,1] = data_vali[:,2] *rad2deg - data_vali[:,14]\n","y_vali[:,2] = data_vali[:,3] - data_vali[:,15] *deg2rad\n","x_vali_scaled = scaler_x_train.transform(x_vali)\n","y_vali_scaled = scaler_y_train.transform(y_vali)\n","\n","\n","x_test_list = []\n","y_test_list = []\n","time_line_list = []\n","ext_jpos_1_list = []\n","ext_jpos_2_list = []\n","ext_jpos_3_list = []\n","rs_jpos_1_list = []\n","rs_jpos_2_list = []\n","rs_jpos_3_list = []\n","rs_jpos_1_lc_list = []\n","rs_jpos_2_lc_list = []\n","rs_jpos_3_lc_list = []\n","for data_test in data_test_list:\n","  time_line = data_test[:,0]\n","  ext_jpos_1 = data_test[:,1] *rad2deg   # joint pose from external joint encoders\n","  ext_jpos_2 = data_test[:,2] *rad2deg\n","  ext_jpos_3 = data_test[:,3]\n","\n","  rs_jpos_1 = data_test[:,13]            # joint pose from ravenstate\n","  rs_jpos_2 = data_test[:,14]\n","  rs_jpos_3 = data_test[:,15] *deg2rad\n","\n","  rs_jpos_1_lc = rs_jpos_1 + bias_jpos_1\n","  rs_jpos_2_lc = rs_jpos_2 + bias_jpos_2\n","  rs_jpos_3_lc = rs_jpos_3 + bias_jpos_3\n","\n","\n","  # add noise for ablation\n","  for noise_idx in noise_select:\n","    if noise_type == 'stdv':\n","      data_test[:, noise_idx] = data_test[:, noise_idx] + np.random.default_rng().uniform(low=-np.std(data_test[:, noise_idx]), high=np.std(data_test[:, noise_idx]), size=data_test[:, noise_idx].shape)\n","    if noise_type == 'mean':\n","      data_test[:, noise_idx] = data_test[:, noise_idx] + np.random.default_rng().uniform(low=-0.5*np.mean(np.abs(data_test[:, noise_idx])), high=0.5*np.mean(np.abs(data_test[:, noise_idx])), size=data_test[:, noise_idx].shape)\n","    if noise_type == 'zero':\n","      data_test[:, noise_idx] = data_test[:, noise_idx] * 0\n","    if noise_type == 'min-max':\n","      range_idx = np.mean(heapq.nlargest(100 ,data_test[:, noise_idx])) - np.mean(heapq.nsmallest(100 ,data_test[:, noise_idx]))\n","      data_test[:, noise_idx] = data_test[:, noise_idx] + np.random.default_rng().uniform(low=-0.5 * range_idx, high=0.5 * range_idx, size=data_test[:, noise_idx].shape)\n","    if noise_type == 'temp':\n","      data_test[:, noise_idx] = data_test[:, noise_idx] * -1\n","\n","  x_test = data_test[:, feature_select]\n","  x_test[:,0] = data_test[:, 0] # change the time stamp to relative time (start with 0)\n","  y_test = data_test[:, 1:4]\n","\n","\n","  x_test_list.append(x_test)\n","  y_test_list.append(y_test)\n","  time_line_list.append(time_line)\n","  ext_jpos_1_list.append(ext_jpos_1)\n","  ext_jpos_2_list.append(ext_jpos_2)\n","  ext_jpos_3_list.append(ext_jpos_3)\n","  rs_jpos_1_list.append(rs_jpos_1)\n","  rs_jpos_2_list.append(rs_jpos_2)\n","  rs_jpos_3_list.append(rs_jpos_3)\n","  rs_jpos_1_lc_list.append(rs_jpos_1_lc)\n","  rs_jpos_2_lc_list.append(rs_jpos_2_lc)\n","  rs_jpos_3_lc_list.append(rs_jpos_3_lc)\n","\n","\n","list_finish_time = []\n","list_rmse_j1_pdt = []\n","list_rmse_j1_rs_lc = []\n","list_rmse_j1_rs = []\n","list_rmse_j2_pdt = []\n","list_rmse_j2_rs_lc = []\n","list_rmse_j2_rs = []\n","list_rmse_j3_pdt = []\n","list_rmse_j3_rs_lc = []\n","list_rmse_j3_rs = []\n","\n","list_mxabse_j1_pdt = []\n","list_mxabse_j1_rs_lc = []\n","list_mxabse_j1_rs = []\n","list_mxabse_j2_pdt = []\n","list_mxabse_j2_rs_lc = []\n","list_mxabse_j2_rs = []\n","list_mxabse_j3_pdt = []\n","list_mxabse_j3_rs_lc = []\n","list_mxabse_j3_rs = []\n","\n","for i in range(0, repeat_times):\n","  np.random.seed(10*i + 3)\n","  tf.random.set_seed(10*i + 3)\n","  tf.keras.utils.set_random_seed(10*i + 3)\n","\n","  dnn_model = None #clean the model\n","########################################################################################################\n","  class dnn_model():\n","      \n","      folder_workspace = None\n","      folder_NN = None\n","      \n","      model = None\n","      \n","      \n","      def set_workspace(self, folder):\n","          self.folder_workspace = folder\n","          self.folder_NN = self.folder_workspace + '/' + 'neural_network'\n","          try:\n","              os.mkdir(self.folder_workspace)\n","          except:\n","              _= 1\n","          try:\n","              os.mkdir(self.folder_NN)\n","          except:\n","              _= 1\n","          return None\n","      \n","      def build_model(self, dim_input, dim_output, hidden_layers_ = [[200],[150],[100]], learning_rate = 0.001):\n","          self.model = Sequential()\n","          \n","          # input layer\n","          self.model.add(tf.keras.Input(shape=(dim_input,)))\n","          \n","          ## add layer normalization\n","          # self.model.add(tf.keras.layers.LayerNormalization(\n","          #                 axis=0,                            # normalize according to columns\n","          #                 epsilon=0.001,\n","          #                 center=True,\n","          #                 scale=True,\n","          #                 beta_initializer=\"zeros\",\n","          #                 gamma_initializer=\"ones\",\n","          #                 beta_regularizer=None,\n","          #                 gamma_regularizer=None,\n","          #                 beta_constraint=None,\n","          #                 gamma_constraint=None,))\n","          \n","          # # add batch normalization\n","          # self.model.add(tf.keras.layers.BatchNormalization(\n","          #                 axis=-1,\n","          #                 momentum=0.99,\n","          #                 epsilon=0.001,\n","          #                 center=True,\n","          #                 scale=True,\n","          #                 beta_initializer=\"zeros\",\n","          #                 gamma_initializer=\"ones\",\n","          #                 moving_mean_initializer=\"zeros\",\n","          #                 moving_variance_initializer=\"ones\",\n","          #                 beta_regularizer=None,\n","          #                 gamma_regularizer=None,\n","          #                 beta_constraint=None,\n","          #                 gamma_constraint=None,\n","          #                 renorm=False,\n","          #                 renorm_clipping=None,\n","          #                 renorm_momentum=0.99,\n","          #                 fused=None,\n","          #                 trainable=True,\n","          #                 virtual_batch_size=None,\n","          #                 adjustment=None,\n","          #                 name=None,))\n","          \n","          # hidden layer\n","          for hidden_layer in hidden_layers_:\n","              self.model.add(Dense(units = hidden_layer[0],\n","                              activation='sigmoid',\n","                              use_bias=True,\n","                              kernel_initializer=\"glorot_uniform\",\n","                              bias_initializer=\"zeros\",\n","                              kernel_regularizer=regularizers.L1L2(l1=kernel_regu_l1, l2=kernel_regu_l2),\n","                              bias_regularizer=regularizers.L2(bias_regu),\n","                              activity_regularizer=regularizers.L2(activity_regu),\n","                              kernel_constraint=None,\n","                              bias_constraint=None,))\n","          \n","          # output layer\n","          self.model.add(Dense(units = dim_output,\n","                              activation='linear',\n","                              use_bias=True,\n","                              kernel_initializer=\"glorot_uniform\",\n","                              bias_initializer=\"zeros\",\n","                              kernel_regularizer=None,\n","                              bias_regularizer=None,\n","                              activity_regularizer=None,\n","                              kernel_constraint=None,\n","                              bias_constraint=None,))\n","          \n","          optimizer_method = tf.keras.optimizers.Adam(learning_rate=learning_rate,\n","                                  beta_1=0.9,\n","                                  beta_2=0.999,\n","                                  epsilon=1e-07,\n","                                  amsgrad=False,\n","                                  name=\"Adam\")\n","          \n","          # loss_function = tf.keras.losses.LogCosh(reduction=\"auto\", name=\"log_cosh\")\n","          \n","          metrics_function = [tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None),\n","                              tf.keras.metrics.MeanAbsolutePercentageError(name=\"mean_absolute_percentage_error\", dtype=None)]\n","          \n","          self.model.compile(optimizer = optimizer_method,\n","                            loss = 'mse',\n","                            metrics = metrics_function,\n","                            loss_weights=None,\n","                            weighted_metrics=None,\n","                            run_eagerly=None)\n","          return None\n","      \n","      def train_model(self, X, y, batch_size, epochs, validation_data=None, early_stop_min_delta = 0, early_stop_patience = 5):\n","          \n","          early_stop = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n","                                                          min_delta = early_stop_min_delta,\n","                                                          patience = early_stop_patience,\n","                                                          verbose=1,\n","                                                          mode=\"auto\",\n","                                                          baseline=None,\n","                                                          restore_best_weights=True)\n","          \n","          History = self.model.fit(x = X,\n","                                  y = y,\n","                                  batch_size=batch_size,\n","                                  epochs = epochs,\n","                                  verbose=0,\n","                                  callbacks=[early_stop],\n","                                  validation_split=0.01,\n","                                  validation_data = validation_data,\n","                                  shuffle=True,\n","                                  class_weight=None,\n","                                  sample_weight=None,\n","                                  initial_epoch=0,\n","                                  steps_per_epoch=None,\n","                                  validation_steps=None,\n","                                  validation_batch_size=None,\n","                                  validation_freq=1,\n","                                  max_queue_size=10,\n","                                  workers=1,\n","                                  use_multiprocessing=False)\n","          \n","          return History\n","      \n","      def evaluate_model(self, X, y):\n","          test_result = self.model.evaluate(x = X,\n","                                              y = y,\n","                                              batch_size=None,\n","                                              verbose=1,\n","                                              sample_weight=None,\n","                                              steps=None,\n","                                              callbacks=None,\n","                                              max_queue_size=10,\n","                                              workers=1,\n","                                              use_multiprocessing=False,\n","                                              return_dict=True)\n","          \n","          return test_result\n","      \n","      def save_model(self, file_path = None):\n","          if file_path == None:\n","              file_path = self.folder_NN + '/' + 'model.h5'\n","          \n","          self.model.save(filepath = file_path,\n","                          overwrite=True,\n","                          include_optimizer=True,\n","                          save_format=None,\n","                          signatures=None,\n","                          options=None)\n","          return None\n","      \n","      def load_model(self, file_path = None):\n","          if file_path == None:\n","              file_path = self.folder_NN + '/' + 'model.h5'\n","          \n","          self.model = tf.keras.models.load_model(filepath = file_path)\n","          return None\n","########################################################################################################\n","\n","  start_time = time.time()\n","  dnn_model = dnn_model()\n","  \n","  dnn_model.set_workspace('r2_learning_calibration/r2_cali_model')\n","\n","  # dnn_model.build_model(dim_input = 17, dim_output = 3, hidden_layers_ = [[600],[500],[400]], learning_rate = 0.001)\n","  dnn_model.build_model(dim_input = feature_dim, dim_output = 3, hidden_layers_ = hidden_layers_, learning_rate = learning_rate)\n","\n","  dnn_model.model.summary()\n","\n","  train_history = dnn_model.train_model(X = x_train_scaled, \n","                                        y = y_train_scaled, \n","                                        batch_size = batch_size, \n","                                        epochs = epochs,\n","                                        validation_data = (x_vali_scaled, y_vali_scaled),\n","                                        early_stop_min_delta = 0, \n","                                        early_stop_patience = 1000)\n","\n","  test_result = dnn_model.evaluate_model(X = x_train_scaled, y = y_train_scaled)\n","  print(test_result)\n","\n","  dnn_model.save_model(file_path = None)\n","\n","  finish_time = time.time() - start_time\n","  print('Training time:')\n","  print(str(time.time() - start_time))\n","  # summarize history for loss\n","  plt.figure()\n","  plt.ylim((0,0.5))\n","  plt.plot(train_history.history['loss'])\n","  plt.plot(train_history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'vali'], loc='upper left')\n","  plt.show()\n","\n","  rmse_j1_pdt_lst = []\n","  rmse_j2_pdt_lst = []\n","  rmse_j3_pdt_lst = []\n","  rmse_j1_rs_lst = []\n","  rmse_j2_rs_lst = []\n","  rmse_j3_rs_lst = []\n","  rmse_j1_rs_lc_lst = []\n","  rmse_j2_rs_lc_lst = []\n","  rmse_j3_rs_lc_lst = []\n","\n","  mxabse_j1_pdt_lst = []\n","  mxabse_j2_pdt_lst = []\n","  mxabse_j3_pdt_lst = []\n","  mxabse_j1_rs_lst = []\n","  mxabse_j2_rs_lst = []\n","  mxabse_j3_rs_lst = []\n","  mxabse_j1_rs_lc_lst = []\n","  mxabse_j2_rs_lc_lst = []\n","  mxabse_j3_rs_lc_lst = []\n","  \n","  idx = 0\n","  for x_test in x_test_list:\n","    y_test = y_test_list[idx]\n","    time_line = time_line_list[idx]\n","    ext_jpos_1 = ext_jpos_1_list[idx]\n","    ext_jpos_2 = ext_jpos_2_list[idx]\n","    ext_jpos_3 = ext_jpos_3_list[idx]\n","    rs_jpos_1 = rs_jpos_1_list[idx]\n","    rs_jpos_2 = rs_jpos_2_list[idx]\n","    rs_jpos_3 = rs_jpos_3_list[idx]\n","    rs_jpos_1_lc = rs_jpos_1_lc_list[idx]\n","    rs_jpos_2_lc = rs_jpos_2_lc_list[idx]\n","    rs_jpos_3_lc = rs_jpos_3_lc_list[idx]\n","    idx += 1\n","  # Evaluate and plot result -----------------------------------------------------\n","    y_predict = dnn_model.model.predict(scaler_x_train.transform(x_test))\n","    y_predict = scaler_y_train.inverse_transform(y_predict)\n","    print(y_predict.shape)\n","\n","    pdt_jpos_1 = rs_jpos_1 + y_predict[:,0] #*rad2deg\n","    pdt_jpos_2 = rs_jpos_2 + y_predict[:,1] #*rad2deg\n","    pdt_jpos_3 = rs_jpos_3 + y_predict[:,2]\n","\n","    rmse_j1_pdt = np.sqrt(np.mean(np.square(pdt_jpos_1 - ext_jpos_1)))\n","    rmse_j2_pdt = np.sqrt(np.mean(np.square(pdt_jpos_2 - ext_jpos_2)))\n","    rmse_j3_pdt = np.sqrt(np.mean(np.square(pdt_jpos_3 - ext_jpos_3)))\n","    rmse_j1_rs = np.sqrt(np.mean(np.square(rs_jpos_1 - ext_jpos_1)))\n","    rmse_j2_rs = np.sqrt(np.mean(np.square(rs_jpos_2 - ext_jpos_2)))\n","    rmse_j3_rs = np.sqrt(np.mean(np.square(rs_jpos_3 - ext_jpos_3)))\n","    rmse_j1_rs_lc = np.sqrt(np.mean(np.square(rs_jpos_1_lc - ext_jpos_1)))\n","    rmse_j2_rs_lc = np.sqrt(np.mean(np.square(rs_jpos_2_lc - ext_jpos_2)))\n","    rmse_j3_rs_lc = np.sqrt(np.mean(np.square(rs_jpos_3_lc - ext_jpos_3)))\n","\n","    mxabse_j1_pdt = np.max(np.abs(pdt_jpos_1 - ext_jpos_1))\n","    mxabse_j2_pdt = np.max(np.abs(pdt_jpos_2 - ext_jpos_2))\n","    mxabse_j3_pdt = np.max(np.abs(pdt_jpos_3 - ext_jpos_3))\n","    mxabse_j1_rs = np.max(np.abs(rs_jpos_1 - ext_jpos_1))\n","    mxabse_j2_rs = np.max(np.abs(rs_jpos_2 - ext_jpos_2))\n","    mxabse_j3_rs = np.max(np.abs(rs_jpos_3 - ext_jpos_3))\n","    mxabse_j1_rs_lc = np.max(np.abs(rs_jpos_1_lc - ext_jpos_1))\n","    mxabse_j2_rs_lc = np.max(np.abs(rs_jpos_2_lc - ext_jpos_2))\n","    mxabse_j3_rs_lc = np.max(np.abs(rs_jpos_3_lc - ext_jpos_3))\n","\n","    rmse_j1_pdt_lst.append(rmse_j1_pdt)\n","    rmse_j2_pdt_lst.append(rmse_j2_pdt)\n","    rmse_j3_pdt_lst.append(rmse_j3_pdt)\n","    rmse_j1_rs_lst.append(rmse_j1_rs)\n","    rmse_j2_rs_lst.append(rmse_j2_rs)\n","    rmse_j3_rs_lst.append(rmse_j3_rs)\n","    rmse_j1_rs_lc_lst.append(rmse_j1_rs_lc)\n","    rmse_j2_rs_lc_lst.append(rmse_j2_rs_lc)\n","    rmse_j3_rs_lc_lst.append(rmse_j3_rs_lc)\n","\n","    mxabse_j1_pdt_lst.append(mxabse_j1_pdt)\n","    mxabse_j2_pdt_lst.append(mxabse_j2_pdt)\n","    mxabse_j3_pdt_lst.append(mxabse_j3_pdt)\n","    mxabse_j1_rs_lst.append(mxabse_j1_rs)\n","    mxabse_j2_rs_lst.append(mxabse_j2_rs)\n","    mxabse_j3_rs_lst.append(mxabse_j3_rs)\n","    mxabse_j1_rs_lc_lst.append(mxabse_j1_rs_lc)\n","    mxabse_j2_rs_lc_lst.append(mxabse_j2_rs_lc)\n","    mxabse_j3_rs_lc_lst.append(mxabse_j3_rs_lc)\n","\n","  list_finish_time.append(finish_time)\n","  list_rmse_j1_pdt.append(rmse_j1_pdt_lst)\n","  list_rmse_j1_rs_lc.append(rmse_j1_rs_lc_lst)\n","  list_rmse_j1_rs.append(rmse_j1_rs_lst)\n","  list_rmse_j2_pdt.append(rmse_j2_pdt_lst)\n","  list_rmse_j2_rs_lc.append(rmse_j2_rs_lc_lst)\n","  list_rmse_j2_rs.append(rmse_j2_rs_lst)\n","  list_rmse_j3_pdt.append(rmse_j3_pdt_lst)\n","  list_rmse_j3_rs_lc.append(rmse_j3_rs_lc_lst)\n","  list_rmse_j3_rs.append(rmse_j3_rs_lst)\n","\n","  list_mxabse_j1_pdt.append(mxabse_j1_pdt_lst)\n","  list_mxabse_j1_rs_lc.append(mxabse_j1_rs_lc_lst)\n","  list_mxabse_j1_rs.append(mxabse_j1_rs_lst)\n","  list_mxabse_j2_pdt.append(mxabse_j2_pdt_lst)\n","  list_mxabse_j2_rs_lc.append(mxabse_j2_rs_lc_lst)\n","  list_mxabse_j2_rs.append(mxabse_j2_rs_lst)\n","  list_mxabse_j3_pdt.append(mxabse_j3_pdt_lst)\n","  list_mxabse_j3_rs_lc.append(mxabse_j3_rs_lc_lst)\n","  list_mxabse_j3_rs.append(mxabse_j3_rs_lst)\n","\n","  print('-------------------')\n","  print('Learning Calibration RMSE Joint 1 (deg):')\n","  print(rmse_j1_pdt_lst)\n","  print('ravenstate RMSE Joint 1 linear corrected (deg):')\n","  print(rmse_j1_rs_lc_lst)\n","  print('ravenstate RMSE Joint 1 (deg):')\n","  print(rmse_j1_rs_lst)\n","  print('-------------------')\n","  print('Learning Calibration RMSE Joint 2 (deg):')\n","  print(rmse_j2_pdt_lst)\n","  print('ravenstate RMSE Joint 2 linear corrected (deg):')\n","  print(rmse_j2_rs_lc_lst)\n","  print('ravenstate RMSE Joint 2 (deg):')\n","  print(rmse_j2_rs_lst)\n","  print('-------------------')\n","  print('Learning Calibration RMSE Joint 3 (mm):')\n","  print(rmse_j3_pdt_lst * 1000)\n","  print('ravenstate RMSE Joint 3 linear corrected (mm):')\n","  print(rmse_j3_rs_lc_lst * 1000)\n","  print('ravenstate RMSE Joint 3 (mm):')\n","  print(rmse_j3_rs_lst * 1000)\n","  print('-------------------')\n","\n","  # fig = plt.figure()\n","  # plt.plot(time_line, ext_jpos_1, 'k')\n","  # plt.plot(time_line, rs_jpos_1, 'b--')\n","  # plt.plot(time_line, pdt_jpos_1, 'r--')\n","  # plt.legend(('gt', 'ravenstate', 'learn_cali'))\n","  # plt.title('Joint 1')\n","\n","  # fig = plt.figure()\n","  # plt.plot(time_line, ext_jpos_2, 'k')\n","  # plt.plot(time_line, rs_jpos_2, 'b--')\n","  # plt.plot(time_line, pdt_jpos_2, 'r--')\n","  # plt.legend(('gt', 'ravenstate', 'learn_cali'))\n","  # plt.title('Joint 2')\n","\n","  # fig = plt.figure()\n","  # plt.plot(time_line, ext_jpos_3, 'k')\n","  # plt.plot(time_line, rs_jpos_3, 'b--')\n","  # plt.plot(time_line, pdt_jpos_3, 'r--')\n","  # plt.legend(('gt', 'ravenstate', 'learn_cali'))\n","  # plt.title('Joint 3')\n","\n","# print final result ----------------------------------------------------------\n","print('################################################################################')\n","print('Final results:')\n","print('Finish time:')\n","print(np.mean(list_finish_time))\n","print(list_finish_time)\n","print('-----------------------------------------------')\n","print('Learning Calibration RMSE Joint 1 (deg):')\n","arr_rmse_j1_pdt = np.array(list_rmse_j1_pdt)\n","for i in range(0,arr_rmse_j1_pdt.shape[1]):\n","  print(np.mean(arr_rmse_j1_pdt[:,i]))\n","print('Learning Calibration SD Joint 1 (deg):')\n","arr_rmse_j1_pdt = np.array(list_rmse_j1_pdt)\n","for i in range(0,arr_rmse_j1_pdt.shape[1]):\n","  print(np.std(arr_rmse_j1_pdt[:,i]))\n","print('Learning Calibration MAX Abs Error Joint 1 (deg):')\n","arr_mxabse_j1_pdt = np.array(list_mxabse_j1_pdt)\n","for i in range(0,arr_mxabse_j1_pdt.shape[1]):\n","  print(np.mean(arr_mxabse_j1_pdt[:,i]))\n","\n","print('ravenstate RMSE Joint 1 linear corrected (deg):')\n","# print(np.mean(list_rmse_j1_rs_lc))\n","arr_rmse_j1_rs_lc = np.array(list_rmse_j1_rs_lc)\n","for i in range(0,arr_rmse_j1_rs_lc.shape[1]):\n","  print(np.mean(arr_rmse_j1_rs_lc[:,i]))\n","print('ravenstate MAX Abs Error Joint 1 linear corrected (deg):')\n","# print(np.mean(list_rmse_j1_rs_lc))\n","arr_mxabse_j1_rs_lc = np.array(list_mxabse_j1_rs_lc)\n","for i in range(0,arr_mxabse_j1_rs_lc.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j1_rs_lc[:,i])))\n","\n","\n","print('ravenstate RMSE Joint 1 (deg):')\n","# print(np.mean(list_rmse_j1_rs))\n","arr_rmse_j1_rs = np.array(list_rmse_j1_rs)\n","for i in range(0,arr_rmse_j1_rs.shape[1]):\n","  print(np.mean(arr_rmse_j1_rs[:,i]))\n","print('ravenstate MAX Abs Error Joint 1 (deg):')\n","# print(np.mean(list_rmse_j1_rs))\n","arr_mxabse_j1_rs = np.array(list_mxabse_j1_rs)\n","for i in range(0,arr_mxabse_j1_rs.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j1_rs[:,i])))\n","\n","print('-----------------------------------------------')\n","print('Learning Calibration RMSE Joint 2 (deg):')\n","# print(np.mean(list_rmse_j2_pdt))\n","arr_rmse_j2_pdt = np.array(list_rmse_j2_pdt)\n","for i in range(0,arr_rmse_j2_pdt.shape[1]):\n","  print(np.mean(arr_rmse_j2_pdt[:,i]))\n","print('Learning Calibration SD Joint 2 (deg):')\n","# print(np.mean(list_rmse_j2_pdt))\n","arr_rmse_j2_pdt = np.array(list_rmse_j2_pdt)\n","for i in range(0,arr_rmse_j2_pdt.shape[1]):\n","  print(np.std(arr_rmse_j2_pdt[:,i]))\n","print('Learning Calibration MAX Abs Error Joint 2 (deg):')\n","arr_mxabse_j2_pdt = np.array(list_mxabse_j2_pdt)\n","for i in range(0,arr_mxabse_j2_pdt.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j2_pdt[:,i])))\n","\n","print('ravenstate RMSE Joint 2 linear corrected (deg):')\n","# print(np.mean(list_rmse_j2_rs_lc))\n","arr_rmse_j2_rs_lc = np.array(list_rmse_j2_rs_lc)\n","for i in range(0,arr_rmse_j2_rs_lc.shape[1]):\n","  print(np.mean(arr_rmse_j2_rs_lc[:,i]))\n","print('ravenstate MAX Abs Error Joint 2 linear corrected (deg):')\n","# print(np.mean(list_rmse_j2_rs_lc))\n","arr_mxabse_j2_rs_lc = np.array(list_mxabse_j2_rs_lc)\n","for i in range(0,arr_mxabse_j2_rs_lc.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j2_rs_lc[:,i])))\n","\n","\n","print('ravenstate RMSE Joint 2 (deg):')\n","# print(np.mean(list_rmse_j2_rs))\n","arr_rmse_j2_rs = np.array(list_rmse_j2_rs)\n","for i in range(0,arr_rmse_j2_rs.shape[1]):\n","  print(np.mean(arr_rmse_j2_rs[:,i]))\n","print('ravenstate MAX Abs Error Joint 2 (deg):')\n","# print(np.mean(list_rmse_j2_rs))\n","arr_mxabse_j2_rs = np.array(list_mxabse_j2_rs)\n","for i in range(0,arr_mxabse_j2_rs.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j2_rs[:,i])))\n","\n","print('-----------------------------------------------')\n","print('Learning Calibration RMSE Joint 3 (mm):')\n","# print(np.mean(list_rmse_j3_pdt))\n","arr_rmse_j3_pdt = np.array(list_rmse_j3_pdt)\n","for i in range(0,arr_rmse_j3_pdt.shape[1]):\n","  print(np.mean(arr_rmse_j3_pdt[:,i]) * 1000)\n","print('Learning Calibration SD Joint 3 (mm):')\n","# print(np.mean(list_rmse_j3_pdt))\n","arr_rmse_j3_pdt = np.array(list_rmse_j3_pdt)\n","for i in range(0,arr_rmse_j3_pdt.shape[1]):\n","  print(np.std(arr_rmse_j3_pdt[:,i]) * 1000)\n","print('Learning Calibration MAX Abs Error Joint 3 (deg):')\n","arr_mxabse_j3_pdt = np.array(list_mxabse_j3_pdt)\n","for i in range(0,arr_mxabse_j3_pdt.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j3_pdt[:,i])) *1000)\n","\n","print('ravenstate RMSE Joint 3 linear corrected (mm):')\n","# print(np.mean(list_rmse_j3_rs_lc))\n","arr_rmse_j3_rs_lc = np.array(list_rmse_j3_rs_lc)\n","for i in range(0,arr_rmse_j3_rs_lc.shape[1]):\n","  print(np.mean(arr_rmse_j3_rs_lc[:,i]) * 1000)\n","print('ravenstate MAX Abs Error Joint 3 linear corrected (mm):')\n","# print(np.mean(list_rmse_j3_rs_lc))\n","arr_mxabse_j3_rs_lc = np.array(list_mxabse_j3_rs_lc)\n","for i in range(0,arr_mxabse_j3_rs_lc.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j3_rs_lc[:,i])) *1000)\n","\n","print('ravenstate RMSE Joint 3 (mm):')\n","# print(np.mean(list_rmse_j3_rs))\n","arr_rmse_j3_rs = np.array(list_rmse_j3_rs)\n","for i in range(0,arr_rmse_j3_rs.shape[1]):\n","  print(np.mean(arr_rmse_j3_rs[:,i]) * 1000)\n","print('ravenstate MAX Abs Error Joint 3 (mm):')\n","# print(np.mean(list_rmse_j3_rs))\n","arr_mxabse_j3_rs = np.array(list_mxabse_j3_rs)\n","for i in range(0,arr_mxabse_j3_rs.shape[1]):\n","  print(np.max(np.abs(arr_mxabse_j3_rs[:,i])) *1000)\n","\n","\n","print('||||||||||||||||||||||||||||||||||||||||||||||||')\n","print('Training Parameters: ')\n","print('Hidden layers: ', hidden_layers_)\n","print('learning_rate:', learning_rate)\n","print('batch_size', batch_size)\n","print('epochs', epochs)\n","print('kernel_regu_l1', kernel_regu_l1)\n","print('kernel_regu_l2', kernel_regu_l2)\n","print('bias_regu', bias_regu)\n","print('activity_regu', activity_regu)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1668386849375,"user":{"displayName":"Haonan Peng","userId":"08169864112056329318"},"user_tz":480},"id":"mw_bhWZm2plv"},"outputs":[],"source":["# sys.exit()\n","# print('active training complete, begin to copy files')\n","# %cp -ar '/content/active_learning_v1' '/content/drive/MyDrive/active_learning_ws/active_learning_v1/test_attempts'\n","# print('Copy files complete')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP2+5SMExp/TK+Jc0QNpZMJ","collapsed_sections":[],"mount_file_id":"1b2AAvgLtagueH7hQLViq0QWhGsGyi1_z","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
